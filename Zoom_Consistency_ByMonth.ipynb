{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1ac703",
   "metadata": {},
   "source": [
    "# Zoom Consistency Report (One Excel • Sheets = Months)\n",
    "\n",
    "This notebook reads your **Insights - Zoom.xlsx** (or **Insights - Zoom - Marketing.xlsx**) and builds a single Excel file where **each sheet is a month** you select.  \n",
    "Within each month sheet, the report lists **all anchors** and **all features**, showing the **direction per bucket** and variability metrics (flip flags, range/std of mean SHAP, etc.).\n",
    "\n",
    "> Change the `months` parameter below to control which sheets to include.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c2eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Choose which workbook to read (pick one)\n",
    "src_path = Path('Insights - Zoom - Marketing.xlsx')\n",
    "\n",
    "# Which month sheets to include in the output (e.g., ['Full'] or ['January','February'])\n",
    "# Use [] to auto-include all sheets from the workbook.\n",
    "months = ['Full']\n",
    "\n",
    "# Output Excel (single file; one sheet per month)\n",
    "out_path = Path('zoomed_in_features/Zoom_Consistency_ByMonth.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598bd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_bucket_order(buckets: pd.Series):\n",
    "    def key(lbl):\n",
    "        if pd.isna(lbl):\n",
    "            return float('inf')\n",
    "        s = str(lbl)\n",
    "        # grab the first number in the label, e.g. \"0 days to 4.23\" -> 0, \"6.46 days+\" -> 6.46\n",
    "        m = re.search(r\"[-+]?\\d*\\.?\\d+(?=\\D|$)\", s)\n",
    "        return float(m.group()) if m else float('inf')\n",
    "    return buckets.map(key)\n",
    "\n",
    "\n",
    "def build_month_report(df_month: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine all anchors for a month into one tall table.\n",
    "    Each row is a (anchor, feature), with bucket direction columns and metrics.\n",
    "    \"\"\"\n",
    "    needed = {'Zoomed In Feature','bucket','feature','direction','mean_shap','mean_abs_shap'}\n",
    "    missing = needed - set(df_month.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns {missing}; did you run the latest Zoom export with direction columns?\")\n",
    "\n",
    "    # Bucket ordering\n",
    "    ordered_buckets = (df_month[['bucket']].drop_duplicates()\n",
    "                       .assign(_order=normalize_bucket_order(df_month['bucket'].drop_duplicates()))\n",
    "                       .sort_values('_order')['bucket'].tolist())\n",
    "\n",
    "    rows = []\n",
    "    for anchor, dfa in df_month.groupby('Zoomed In Feature'):\n",
    "        # pivot directions across buckets for this anchor\n",
    "        pivot_dir = dfa.pivot_table(index='feature', columns='bucket', values='direction', aggfunc='first')\n",
    "        pivot_dir = pivot_dir.reindex(columns=ordered_buckets)\n",
    "\n",
    "        grp = dfa.groupby('feature')\n",
    "        n_buckets_present = grp['bucket'].nunique()\n",
    "        n_unique_dir = grp['direction'].nunique()\n",
    "\n",
    "        def has_flip(series):\n",
    "            vals = set(series.dropna().astype(str))\n",
    "            return int(('Up' in vals) and ('Down' in vals))\n",
    "\n",
    "        flip_flag = grp['direction'].apply(has_flip)\n",
    "        std_mean = grp['mean_shap'].std(ddof=1)\n",
    "        rng_mean = grp['mean_shap'].agg(lambda s: (s.max() - s.min()) if len(s)>0 else np.nan)\n",
    "        avg_abs  = grp['mean_abs_shap'].mean()\n",
    "\n",
    "        metrics = pd.DataFrame({\n",
    "            'n_buckets_present': n_buckets_present,\n",
    "            'n_unique_directions': n_unique_dir,\n",
    "            'has_flip_Up_Down': flip_flag,\n",
    "            'std_mean_shap': std_mean,\n",
    "            'range_mean_shap': rng_mean,\n",
    "            'avg_mean_abs_shap': avg_abs\n",
    "        })\n",
    "\n",
    "        report = pivot_dir.merge(metrics, left_index=True, right_index=True, how='left')\n",
    "        # add anchor context\n",
    "        report.insert(0, 'anchor', anchor)\n",
    "\n",
    "        # consistency label\n",
    "        def consistency_row(row):\n",
    "            if row['has_flip_Up_Down'] == 1:\n",
    "                return 'Variable (flip)'\n",
    "            if row['n_unique_directions'] == 1:\n",
    "                return 'Consistent'\n",
    "            return 'Variable'\n",
    "        report['consistency'] = report.apply(consistency_row, axis=1)\n",
    "\n",
    "        # sort interesting first\n",
    "        report = report.sort_values(\n",
    "            by=['has_flip_Up_Down','range_mean_shap','std_mean_shap','avg_mean_abs_shap'],\n",
    "            ascending=[False, False, False, False]\n",
    "        ).reset_index().rename(columns={'index':'feature'})\n",
    "\n",
    "        rows.append(report)\n",
    "\n",
    "    # stack anchors into one sheet\n",
    "    final = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bf3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Built report for month: Full  (rows=1740)\n",
      "✅ Wrote: zoomed_in_features/Zoom_Consistency_ByMonth.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Build the single Excel with one sheet per selected month ===\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "assert src_path.exists(), f\"Workbook not found: {src_path}\"\n",
    "xls = pd.ExcelFile(src_path)\n",
    "all_sheets = xls.sheet_names\n",
    "\n",
    "# If months list is empty, include all sheets\n",
    "selected_months = months if months else all_sheets\n",
    "\n",
    "# Build all selected month reports first\n",
    "month_reports = {}\n",
    "for m in selected_months:\n",
    "    if m not in all_sheets:\n",
    "        print(f\"⚠️ Sheet '{m}' not found in {src_path.name}; skipping.\")\n",
    "        continue\n",
    "    dfm = pd.read_excel(src_path, sheet_name=m)\n",
    "    try:\n",
    "        rep = build_month_report(dfm)\n",
    "        month_reports[m] = rep\n",
    "        print(f\"✓ Built report for month: {m}  (rows={len(rep)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped '{m}' due to: {e}\")\n",
    "\n",
    "if not month_reports:\n",
    "    raise RuntimeError('No month reports built. Check your `months` list and the workbook structure.')\n",
    "\n",
    "# Write to one Excel (one sheet per month)\n",
    "if out_path.exists():\n",
    "    out_path.unlink()\n",
    "\n",
    "with pd.ExcelWriter(out_path, engine='openpyxl', mode='w') as writer:\n",
    "    # Optional: Index sheet\n",
    "    idx = pd.DataFrame({\n",
    "        'month': list(month_reports.keys()),\n",
    "        'rows': [len(v) for v in month_reports.values()]\n",
    "    })\n",
    "    idx.to_excel(writer, sheet_name='Index', index=False)\n",
    "\n",
    "    for m, rep in month_reports.items():\n",
    "        sheet = m[:31]  # Excel limit\n",
    "        rep.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"✅ Wrote: {out_path.as_posix()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f126395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QMVA_Regression_Findings_Marketing_QUANT_v7.docx\n"
     ]
    }
   ],
   "source": [
    "# QMVA Findings — centered images layout (v7, patched with signed value phrases + bold section headers)\n",
    "# Output: /mnt/data/QMVA_Regression_Findings_Marketing_QUANT_v7.docx\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "# ---------- Paths ----------\n",
    "BASE = Path(\"\")\n",
    "LOGO = BASE/\"walker_logo.png\"\n",
    "INSIGHTS = BASE/\"Insights.xlsx\"\n",
    "ZOOM = BASE/\"Insights - Zoom - Marketing.xlsx\"\n",
    "SHAP_SUMMARY = BASE/\"multivariate_modeling_results/shap/Full_SHAP_Summary_Plot.png\"\n",
    "ZOOM_DIR = BASE/\"multivariate_modeling_results/shap_zoom/Full\"\n",
    "OUT = BASE/\"QMVA_Regression_Findings_Marketing_QUANT_v7.docx\"\n",
    "\n",
    "MONTH = \"Full\"\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def prettify(s: str) -> str:\n",
    "    if not isinstance(s,str): return str(s)\n",
    "    s = re.sub(r\"\\s+\",\" \", s.replace(\"_\",\" \").strip())\n",
    "    return \" \".join(tok if (tok.isupper() and len(tok)<=6) else tok.capitalize() for tok in s.split())\n",
    "\n",
    "def strip_units(lbl: str) -> str:\n",
    "    return re.sub(r\"\\b(days?|hrs?|hours?)\\b\", \"\", str(lbl), flags=re.IGNORECASE).strip()\n",
    "\n",
    "def normalize_bucket_order(df: pd.DataFrame):\n",
    "    def key(lbl):\n",
    "        if pd.isna(lbl): return float('inf')\n",
    "        m = re.search(r\"[-+]?\\d*\\.?\\d+(?=\\D|$)\", str(lbl))\n",
    "        return float(m.group()) if m else float('inf')\n",
    "    return (df[[\"bucket\"]].drop_duplicates()\n",
    "            .assign(_o=lambda d: d[\"bucket\"].map(key))\n",
    "            .sort_values(\"_o\"))[\"bucket\"].tolist()\n",
    "\n",
    "def set_justified_style(doc: Document):\n",
    "    for nm in ['Normal','Heading 1','Heading 2','Heading 3']:\n",
    "        try:\n",
    "            st = doc.styles[nm]\n",
    "            st.font.name = 'Calibri'\n",
    "            st.font.size = Pt(11 if nm=='Normal' else (18 if nm=='Heading 1' else 14))\n",
    "            if nm=='Normal':\n",
    "                st.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "                st.paragraph_format.space_after = Pt(6)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "def set_one_inch_margins(doc: Document):\n",
    "    for sec in doc.sections:\n",
    "        sec.top_margin = sec.bottom_margin = sec.left_margin = sec.right_margin = Inches(1)\n",
    "\n",
    "def driver_strength_text(pos: int) -> str:\n",
    "    if pos == 1: return \"This is the highest driver in this period.\"\n",
    "    if pos and pos <= 3: return \"This is a strong driver in this period.\"\n",
    "    if pos and pos <= 10: return \"This feature is a moderate driver in this period.\"\n",
    "    return \"This feature is a minor driver in this period.\"\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"-\", s.lower()).strip(\"-\")\n",
    "\n",
    "def _feature_name_variants(anchor: str):\n",
    "    raw = anchor\n",
    "    with_unders = anchor.replace(\" \", \"_\")\n",
    "    pretty = prettify(anchor).replace(\" \", \"_\")\n",
    "    no_dunders = re.sub(r\"_+\", \"_\", with_unders)\n",
    "    return {raw, with_unders, pretty, no_dunders}\n",
    "\n",
    "def find_zoom_image_for_anchor(anchor: str, zoom_dir: Path) -> Path | None:\n",
    "    if not zoom_dir.exists(): return None\n",
    "    files = list(zoom_dir.glob(\"*.png\"))\n",
    "    if not files: return None\n",
    "    norm_to_path = { _norm(p.stem): p for p in files }\n",
    "    for variant in _feature_name_variants(anchor):\n",
    "        cand = f\"Full_{variant}_ALL_BUCKETS_side_by_side\"\n",
    "        if _norm(cand) in norm_to_path:\n",
    "            return norm_to_path[_norm(cand)]\n",
    "    anchor_norm = _norm(anchor)\n",
    "    candidates = [p for p in files if anchor_norm in _norm(p.stem)]\n",
    "    def weight(p: Path):\n",
    "        s = _norm(p.stem); score = 0\n",
    "        if \"all-buckets\" in s: score += 3\n",
    "        if \"side-by-side\" in s: score += 2\n",
    "        if s.startswith(\"full-\"): score += 1\n",
    "        return score\n",
    "    return sorted(candidates, key=weight, reverse=True)[0] if candidates else None\n",
    "\n",
    "# ---------- NEW: signed value phrase helper ----------\n",
    "def value_phrase_signed(feature_name: str, mean: float) -> str:\n",
    "    \"\"\"\n",
    "    Return a short phrase that names the value that’s implied by the feature name,\n",
    "    flipping to 'not <value>' when the mean SHAP is negative (i.e., that named value lowers).\n",
    "    \"\"\"\n",
    "    f = feature_name.strip()\n",
    "\n",
    "    # Gender M/F → flip to the opposite when mean < 0\n",
    "    m = re.search(r'Consumer[_ ]?Gender[_ ]?([MF])$', f, flags=re.I)\n",
    "    if m:\n",
    "        val = m.group(1).upper()\n",
    "        neg = 'F' if val == 'M' else 'M'\n",
    "        return f\"(when Gender = {val})\" if mean >= 0 else f\"(when Gender = {neg})\"\n",
    "\n",
    "    # *_Flag_Yes / *_Flag_No (treat 'Yes' as the named value)\n",
    "    if re.search(r'(^|_)Flag($|_)', f, flags=re.I):\n",
    "        if re.search(r'(^|_)Yes($|_)', f, flags=re.I):\n",
    "            base = prettify(re.sub(r'[_ ]?Flag[_ ]?Yes', '', f, flags=re.I)).strip() or prettify(f)\n",
    "            return f\"(when {base} = Yes)\" if mean >= 0 else f\"(when {base} = No)\"\n",
    "        if re.search(r'(^|_)No($|_)', f, flags=re.I):\n",
    "            base = prettify(re.sub(r'[_ ]?Flag[_ ]?No', '', f, flags=re.I)).strip() or prettify(f)\n",
    "            return f\"(when {base} = No)\" if mean >= 0 else f\"(when {base} = Yes)\"\n",
    "\n",
    "    # Terminal _Yes / _No\n",
    "    m2 = re.search(r'(_Yes|_No)$', f, flags=re.I)\n",
    "    if m2:\n",
    "        base = prettify(re.sub(r'(_Yes|_No)$','', f, flags=re.I))\n",
    "        val = m2.group(1).replace('_','').title()\n",
    "        other = \"No\" if val == \"Yes\" else \"Yes\"\n",
    "        return f\"(when {base} = {val})\" if mean >= 0 else f\"(when {base} = {other})\"\n",
    "\n",
    "    # TimeOfDay_Night dummy\n",
    "    if re.search(r'TimeOfDay[_ ]?Night$', f, flags=re.I):\n",
    "        return \"(when Night = Yes)\" if mean >= 0 else \"(when it is not Night)\"\n",
    "\n",
    "    # Season one-hots (… = Summer, flip to 'not Summer')\n",
    "    sm = re.search(r'(Spring|Summer|Winter|Fall)$', f, flags=re.I)\n",
    "    if sm and re.search(r'(Season|Call[_ ]?Season)', f, flags=re.I):\n",
    "        season = sm.group(1).title()\n",
    "        base = prettify(re.sub(r'(Spring|Summer|Winter|Fall)$','', f, flags=re.I)).strip()\n",
    "        return f\"(when {base} = {season})\" if mean >= 0 else f\"(when it is not {season})\"\n",
    "\n",
    "    # Geo/brand codes: treat suffix as the named category; flip to 'not <cat>' when mean < 0\n",
    "    m3 = re.search(r'_([A-Z0-9]+(?:_[A-Z0-9]+)*)$', f)\n",
    "    if m3 and len(m3.group(1)) <= 12:\n",
    "        base = prettify(re.sub(r'_([A-Z0-9]+(?:_[A-Z0-9]+)*)$','', f))\n",
    "        cat = m3.group(1).replace('_',' ')\n",
    "        return f\"(when {base} = {cat})\" if mean >= 0 else f\"(when not {cat})\"\n",
    "\n",
    "    # Fallback (continuous/unknown) — no side specified\n",
    "    return \"\"\n",
    "\n",
    "# ---------- Load data ----------\n",
    "ins = pd.read_excel(INSIGHTS, sheet_name=MONTH)\n",
    "zoom = pd.read_excel(ZOOM, sheet_name=MONTH)\n",
    "\n",
    "# Rank ordering\n",
    "if {\"Zoomed In Feature\",\"Zoomed In Feature Position\"}.issubset(zoom.columns):\n",
    "    rank_df = (zoom.groupby(\"Zoomed In Feature\")[\"Zoomed In Feature Position\"]\n",
    "                 .min().reset_index()\n",
    "                 .sort_values(\"Zoomed In Feature Position\"))\n",
    "else:\n",
    "    rank_df = ins.sort_values(\"mean_abs_shap\", ascending=False)[[\"feature\"]].rename(columns={\"feature\":\"Zoomed In Feature\"})\n",
    "    rank_df[\"Zoomed In Feature Position\"] = range(1, len(rank_df)+1)\n",
    "\n",
    "total_abs = float(ins[\"mean_abs_shap\"].sum())\n",
    "top5 = ins.sort_values(\"mean_abs_shap\", ascending=False).head(5)[[\"feature\",\"mean_abs_shap\"]].copy()\n",
    "top5[\"share\"] = top5[\"mean_abs_shap\"]/total_abs*100.0 if total_abs>0 else 0.0\n",
    "\n",
    "def intro_for_feature(feat: str):\n",
    "    row = ins[ins[\"feature\"]==feat]\n",
    "    if row.empty:\n",
    "        row = ins[ins[\"feature\"].str.replace(\" \",\"_\")==feat]\n",
    "    if row.empty:\n",
    "        return None\n",
    "    r = row.iloc[0]\n",
    "    share = (r[\"mean_abs_shap\"]/total_abs*100.0) if total_abs>0 else np.nan\n",
    "    pct_pos = float(r.get(\"pct_positive_shap\", np.nan))*100.0 if not pd.isna(r.get(\"pct_positive_shap\", np.nan)) else np.nan\n",
    "    direction = \"upward\" if r[\"mean_shap\"]>0 else (\"downward\" if r[\"mean_shap\"]<0 else \"balanced\")\n",
    "    return dict(mean_abs=float(r[\"mean_abs_shap\"]),\n",
    "                share=float(share) if share==share else np.nan,\n",
    "                direction=direction,\n",
    "                pct_pos=float(pct_pos) if pct_pos==pct_pos else np.nan)\n",
    "\n",
    "# ---------- Bucket narratives ----------\n",
    "def bucket_metrics(dfa: pd.DataFrame, anchor_feat: str):\n",
    "    dfa = dfa.copy()\n",
    "    if \"direction\" not in dfa.columns:\n",
    "        eps = 1e-9\n",
    "        dfa[\"direction\"] = np.where(dfa[\"mean_shap\"] > eps, \"Up\",\n",
    "                            np.where(dfa[\"mean_shap\"] < -eps, \"Down\", \"Neutral\"))\n",
    "    tot_per_bucket = dfa.groupby(\"bucket\")[\"n_rows\"].max()\n",
    "    ordered = normalize_bucket_order(dfa)\n",
    "    sub = dfa[dfa[\"feature\"] != anchor_feat]\n",
    "    agg = (sub.groupby([\"bucket\", \"feature\"])\n",
    "              .agg(mean_shap=(\"mean_shap\", \"mean\"),\n",
    "                   mean_abs_shap=(\"mean_abs_shap\", \"mean\"),\n",
    "                   n_rows=(\"n_rows\", \"max\"),\n",
    "                   direction=(\"direction\", lambda s: s.mode().iat[0] if len(s.mode()) else \"Neutral\"))\n",
    "              .reset_index())\n",
    "    agg[\"bucket_rows\"] = agg[\"bucket\"].map(tot_per_bucket).astype(float)\n",
    "    agg[\"bucket_share\"] = np.where(agg[\"bucket_rows\"] > 0, agg[\"n_rows\"]/agg[\"bucket_rows\"], np.nan)\n",
    "    ordered_c = pd.Categorical(agg[\"bucket\"], categories=ordered, ordered=True)\n",
    "    agg = agg.assign(__bkey__=ordered_c).sort_values([\"__bkey__\", \"mean_abs_shap\"], ascending=[True, False]).drop(columns=\"__bkey__\")\n",
    "    return agg, ordered, tot_per_bucket\n",
    "\n",
    "def bucket_level_narrative(dfa: pd.DataFrame, anchor_feat: str, top_k=3):\n",
    "    agg, ordered, btot = bucket_metrics(dfa, anchor_feat)\n",
    "    lines = []\n",
    "    for b in ordered:\n",
    "        slab = agg[agg[\"bucket\"] == b]\n",
    "        if slab.empty: continue\n",
    "        b_label = strip_units(b)\n",
    "        bucket_n = float(btot.get(b, np.nan) or np.nan)\n",
    "        anchor_total = float(btot.sum())\n",
    "        share = (bucket_n / anchor_total) if anchor_total > 0 else np.nan\n",
    "\n",
    "        up = slab[slab[\"direction\"] == \"Up\"].nlargest(top_k, \"mean_abs_shap\")[[\"feature\", \"mean_shap\"]]\n",
    "        dn = slab[slab[\"direction\"] == \"Down\"].nlargest(top_k, \"mean_abs_shap\")[[\"feature\", \"mean_shap\"]]\n",
    "\n",
    "        def fmt_pair(feat, mean):\n",
    "            phr = value_phrase_signed(str(feat), float(mean))\n",
    "            phr = f\" {phr}\" if phr else \"\"\n",
    "            return f\"{prettify(feat)}{phr} (≈{mean:+.2f})\"\n",
    "\n",
    "        parts = []\n",
    "        if len(up):\n",
    "            parts.append(\", \".join([fmt_pair(f, m) for f, m in up.itertuples(index=False)]) + \" tend to increase the prediction\")\n",
    "        if len(dn):\n",
    "            parts.append(\", \".join([fmt_pair(f, m) for f, m in dn.itertuples(index=False)]) + \" tend to decrease the prediction\")\n",
    "\n",
    "        lines.append(f\"• In bucket ‘{b_label}’ (~{share*100:0.0f}% of this anchor’s data), \" + \" and \".join(parts) + \".\") if parts \\\n",
    "             else lines.append(f\"• In bucket ‘{b_label}’, effects are mixed; no clear pattern stands out.\")\n",
    "    return lines\n",
    "\n",
    "def bucket_differences_narrative(dfa: pd.DataFrame, anchor_feat: str, max_items=8):\n",
    "    agg, ordered, _ = bucket_metrics(dfa, anchor_feat)\n",
    "    if agg.empty: return [\"• No clear bucket-level differences found.\"]\n",
    "\n",
    "    lines = []\n",
    "    dir_piv = agg.pivot_table(index=\"feature\", columns=\"bucket\", values=\"direction\", aggfunc=\"first\").reindex(columns=ordered)\n",
    "    val_piv = agg.pivot_table(index=\"feature\", columns=\"bucket\", values=\"mean_shap\", aggfunc=\"mean\").reindex(columns=ordered)\n",
    "\n",
    "    def clean(lbl): return strip_units(lbl)\n",
    "\n",
    "    flips = []\n",
    "    for feat, row in dir_piv.iterrows():\n",
    "        bnames = [b for b in row.index if pd.notna(row[b])]\n",
    "        for i in range(len(bnames)-1):\n",
    "            d1, d2 = row[bnames[i]], row[bnames[i+1]]\n",
    "            if d1 in {\"Up\",\"Down\"} and d2 in {\"Up\",\"Down\"} and d1 != d2:\n",
    "                v1, v2 = val_piv.loc[feat, bnames[i]], val_piv.loc[feat, bnames[i+1]]\n",
    "                if pd.isna(v1) or pd.isna(v2): continue\n",
    "                delta = abs(v2 - v1)\n",
    "                denom = abs(v1) + abs(v2)\n",
    "                pct_dev = (delta/denom*100.0) if denom > 1e-9 else np.nan\n",
    "                flips.append((feat, bnames[i], bnames[i+1], d1, d2, delta, pct_dev))\n",
    "\n",
    "    flips = sorted(flips, key=lambda t: t[5], reverse=True)[:max_items]\n",
    "    for feat, b1, b2, d1, d2, delta, pct_dev in flips:\n",
    "        pct_txt = f\"{pct_dev:.0f}%\" if pd.notna(pct_dev) else \"—\"\n",
    "        # Use the sign of the *second* bucket to decide the phrase; either is fine since we describe both\n",
    "        mean2 = val_piv.loc[feat, b2]\n",
    "        phr = value_phrase_signed(str(feat), float(mean2))\n",
    "        phr = f\" {phr}\" if phr else \"\"\n",
    "        lines.append(\n",
    "            f\"• {prettify(feat)}{phr} increases in bucket ‘{clean(b1)}’ but decreases in bucket ‘{clean(b2)}’. \"\n",
    "            f\"The difference is about {delta:.2f} SHAP points — roughly a {pct_txt} swing between these two buckets.\"\n",
    "        )\n",
    "\n",
    "    return lines or [\"• No meaningful flips between buckets for this feature.\"]\n",
    "\n",
    "# ---------- Layout helper ----------\n",
    "def add_center_image(doc: Document, path: Path, width_inches=4.0):\n",
    "    if path and path.exists():\n",
    "        p = doc.add_paragraph(); p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        p.add_run().add_picture(str(path), width=Inches(width_inches))\n",
    "\n",
    "# ---------- Build DOCX ----------\n",
    "doc = Document()\n",
    "set_one_inch_margins(doc)\n",
    "set_justified_style(doc)\n",
    "\n",
    "# Cover\n",
    "if LOGO.exists():\n",
    "    p = doc.add_paragraph(); p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    p = doc.add_paragraph(); p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    p.add_run().add_picture(str(LOGO), width=Inches(2.8))\n",
    "p = doc.add_paragraph(); p.alignment = WD_ALIGN_PARAGRAPH.CENTER; p.style = doc.styles['Heading 1']\n",
    "p.add_run(\"Regression Findings (Marketing)\")\n",
    "p = doc.add_paragraph(); p.alignment = WD_ALIGN_PARAGRAPH.CENTER; p.style = doc.styles['Heading 2']\n",
    "p.add_run(\"Predicting QMVA — period overview and feature interactions\")\n",
    "doc.add_paragraph(f\"Sources: {INSIGHTS.name} (overall), {ZOOM.name} (zoom buckets)\")\n",
    "doc.add_page_break()\n",
    "\n",
    "# Period Overview\n",
    "p = doc.add_paragraph(); p.style = doc.styles['Heading 1']; p.add_run(\"Period overview\")\n",
    "add_center_image(doc, SHAP_SUMMARY)\n",
    "doc.add_paragraph(\n",
    "    \"Total SHAP impact this period reflects the combined importance of all features. \"\n",
    "    \"Higher values mean the model’s predictions are driven more strongly by the inputs.\"\n",
    ")\n",
    "doc.add_paragraph(f\"Overall total impact (sum of mean |SHAP| across features): approximately {total_abs:.2f}.\")\n",
    "doc.add_paragraph(\"Top contributors this period:\")\n",
    "for _, r in top5.iterrows():\n",
    "    doc.add_paragraph(f\"• {prettify(r['feature'])} — about {r['share']:.1f}% of total impact\")\n",
    "doc.add_page_break()\n",
    "\n",
    "# Operational Definitions\n",
    "p = doc.add_paragraph(); p.style = doc.styles['Heading 1']; p.add_run(\"Operational definitions\")\n",
    "defs = [\n",
    "    (\"SHAP value\",\"Signed contribution: positive raises the prediction; negative lowers it.\"),\n",
    "    (\"Mean |SHAP| (impact)\",\"Average absolute SHAP: overall influence, regardless of sign.\"),\n",
    "    (\"Direction (Up/Down)\",\"Net tendency within a bucket: Up usually raises; Down lowers.\"),\n",
    "    (\"Bucket\",\"A slice of the anchor feature’s range (e.g., ranges of Log Days Since Incident).\"),\n",
    "]\n",
    "for t, d in defs: doc.add_paragraph(f\"• {t}: {d}\")\n",
    "doc.add_page_break()\n",
    "\n",
    "# Feature index\n",
    "p = doc.add_paragraph(); p.style = doc.styles['Heading 1']; p.add_run(\"Feature index (this period)\")\n",
    "for _, rr in rank_df.iterrows():\n",
    "    feat = str(rr[\"Zoomed In Feature\"]); pos = int(rr[\"Zoomed In Feature Position\"])\n",
    "    doc.add_paragraph(f\"Top {pos} — {prettify(feat)}\")\n",
    "doc.add_page_break()\n",
    "\n",
    "# Detailed sections\n",
    "for _, rr in rank_df.iterrows():\n",
    "    anchor = str(rr[\"Zoomed In Feature\"]); pos = int(rr[\"Zoomed In Feature Position\"])\n",
    "\n",
    "    h = doc.add_paragraph(); h.style = doc.styles['Heading 2']; h.add_run(f\"{prettify(anchor)} — this period\")\n",
    "    doc.add_paragraph(driver_strength_text(pos))\n",
    "\n",
    "    add_center_image(doc, find_zoom_image_for_anchor(anchor, ZOOM_DIR))\n",
    "\n",
    "    st = intro_for_feature(anchor)\n",
    "    if st:\n",
    "        share_txt = f\"{st['share']:.1f}%\" if st['share']==st['share'] else \"—\"\n",
    "        pos_txt = f\"{st['pct_pos']:.0f}%\" if st['pct_pos']==st['pct_pos'] else \"—\"\n",
    "        doc.add_paragraph(\n",
    "            f\"Overall: this feature accounts for about {share_txt} of total model impact this period. \"\n",
    "            f\"The net tendency is {st['direction']} (around {pos_txt} of cases push upward).\"\n",
    "        )\n",
    "    else:\n",
    "        doc.add_paragraph(\"Overall: impact summary available qualitatively; quantitative detail not found for this feature.\")\n",
    "\n",
    "    dfa = zoom[zoom[\"Zoomed In Feature\"]==anchor].copy()\n",
    "    if \"direction\" not in dfa.columns and \"mean_shap\" in dfa.columns:\n",
    "        eps = 1e-9\n",
    "        dfa[\"direction\"] = np.where(dfa[\"mean_shap\"]>eps,\"Up\",np.where(dfa[\"mean_shap\"]<-eps,\"Down\",\"Neutral\"))\n",
    "\n",
    "    # --- Bold subsection header: per-bucket ---\n",
    "    p = doc.add_paragraph(); run = p.add_run(\"How this feature behaves across buckets\"); run.bold = True\n",
    "    for line in bucket_level_narrative(dfa, anchor, top_k=3):\n",
    "        doc.add_paragraph(line)\n",
    "\n",
    "    # --- Bold subsection header: changes ---\n",
    "    p = doc.add_paragraph(); run = p.add_run(\"Where behavior changes across buckets (with magnitude)\"); run.bold = True\n",
    "    for line in bucket_differences_narrative(dfa, anchor, max_items=8):\n",
    "        doc.add_paragraph(line)\n",
    "\n",
    "    doc.add_page_break()\n",
    "\n",
    "doc.save(str(OUT))\n",
    "print(str(OUT))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
